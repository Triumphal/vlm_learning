{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88dd5582",
   "metadata": {},
   "source": [
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3aaa05",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "hf download liuhaotian/LLaVA-CC3M-Pretrain-595K --repo-type=dataset --local-dir ../../../../datasets/LLaVA-CC3M-Pretrain-595K\n",
    "hf download CaptionEmporium/TextOCR-GPT4o --repo-type=dataset --local-dir ../../../../datasets/TextOCR-GPT4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d8f7a",
   "metadata": {},
   "source": [
    "## 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c63d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List\n",
    "from transformers import AutoProcessor,LlavaProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4153cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/LLaVA-CC3M-Pretrain-595K\"\n",
    "data_dir\n",
    "\n",
    "chat_file = Path(data_dir,\"chat.json\")\n",
    "chat_data = pd.read_json(path_or_buf=chat_file)\n",
    "# chat_data.shape\n",
    "# chat_data.head(20)\n",
    "# chat_data.iloc[10][\"conversations\"][1][\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52a0de",
   "metadata": {},
   "source": [
    "## 构建数据集类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90570654",
   "metadata": {},
   "source": [
    "### 1、构建llava读取数据集的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41125e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir) -> None:\n",
    "        super().__init__()\n",
    "        self.chat_data, self.image_dir = self.build_dataset(data_dir=data_dir)\n",
    "\n",
    "    def build_dataset(self, data_dir) -> tuple[list, Path]:\n",
    "        data_dir = Path(data_dir)\n",
    "        chat_file = data_dir.joinpath(\"chat.json\")\n",
    "        images_dir = data_dir.joinpath(\"images\")\n",
    "\n",
    "        chat_data = pd.read_json(path_or_buf=chat_file).to_dict(\"records\")\n",
    "        return chat_data, images_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chat_data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, str, Path]:\n",
    "        cur_data = self.chat_data[index]\n",
    "        humen_input = cur_data[\"conversations\"][0][\"value\"]\n",
    "        gpt_output = cur_data[\"conversations\"][1][\"value\"]\n",
    "        image_path = self.image_dir.joinpath(cur_data.get(\"image\"))\n",
    "        return (humen_input, gpt_output, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e741081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Relay a brief, clear account of the picture shown.\\n<image>',\n",
       " 'e and person vintage initials logo symbol .',\n",
       " PosixPath('datasets/LLaVA-CC3M-Pretrain-595K/images/GCC_train_001015674.jpg'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"datasets/LLaVA-CC3M-Pretrain-595K\"\n",
    "test_llavadataset = LlavaDataset(data_dir)\n",
    "test_llavadataset[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3dbcec",
   "metadata": {},
   "source": [
    "### 2、单个输入处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbad56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_model_path = \"models/llava_clip-L-14-336_Qwen1.5-1.8B\"\n",
    "llava_process = LlavaProcessor.from_pretrained(llava_model_path,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c85050",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_process.tokenizer(\n",
    "    \"a_text\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    "    truncation=True\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded11d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Relay a brief, clear account of the picture shown.\\n<image>',\n",
       " 'e and person vintage initials logo symbol .',\n",
       " PosixPath('datasets/LLaVA-CC3M-Pretrain-595K/images/GCC_train_001015674.jpg'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_llavadataset[123]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b2ca6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QaImagaOutput:\n",
    "    q_input_ids: torch.Tensor\n",
    "    pixel_values: torch.Tensor\n",
    "    a_input_ids: torch.Tensor\n",
    "\n",
    "\n",
    "def build_qaimage(\n",
    "    processor: LlavaProcessor, q_text: str, a_text: str, image_path: Path\n",
    "):\n",
    "\n",
    "    # 千问的对话模板\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": q_text},\n",
    "    ]\n",
    "    prompt = processor.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,  # 是否直接返回token ID（默认False，返回字符串）\n",
    "        add_generation_prompt=True,  # 是否在末尾添加生成提示（如\"Assistant:\"）\n",
    "    )\n",
    "    image_file = image_path\n",
    "    raw_image = Image.open(image_file)\n",
    "\n",
    "    inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\")\n",
    "\n",
    "    a_input_ids = processor.tokenizer(\n",
    "        a_text, return_tensors=\"pt\", padding=\"longest\", truncation=True\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    return QaImagaOutput(\n",
    "        q_input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        a_input_ids=a_input_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc0c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = build_qaimage(llava_process, test_data[0], test_data[1], test_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a5321f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 607])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.q_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afdf64c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151645]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(llava_process.tokenizer.eos_token_ids).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2aa72",
   "metadata": {},
   "source": [
    "### 3、多batch整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54bfa9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLlavaModelCollator:\n",
    "    def __init__(self, processor: LlavaProcessor, IGNORE_INDEX: int = -100):\n",
    "        self.processor = processor\n",
    "        self.ignore_index = IGNORE_INDEX\n",
    "        self.eos_token_ids = self.processor.tokenizer.eos_token_ids\n",
    "        self.pad_token_ids = self.processor.tokenizer.pad_token_ids\n",
    "\n",
    "    def convert_one_piece(self, q_input_ids: torch.Tensor, a_input_ids: torch.Tensor):\n",
    "        input_ids = torch.concat(\n",
    "            [\n",
    "                q_input_ids,\n",
    "                a_input_ids,\n",
    "                torch.tensor(self.eos_token_ids).reshape(1, -1),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        # labels 用来控制是否计算loss\n",
    "        labels = torch.concat(\n",
    "            [\n",
    "                torch.full_like(q_input_ids, fill_value=self.ignore_index),\n",
    "                a_input_ids,\n",
    "                torch.tensor(self.eos_token_ids).reshape(1, -1),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        return input_ids, labels\n",
    "\n",
    "    def __call__(self, features: List):\n",
    "        input_ids_list = []\n",
    "        labels_list = []\n",
    "        pixel_values_list = []\n",
    "        max_input_len_list = []\n",
    "        for feature in features:\n",
    "            qaimage_output: QaImagaOutput = build_qaimage(\n",
    "                self.processor, feature[0], feature[1], feature[2]\n",
    "            )\n",
    "            temp_input_ids, temp_labels = self.convert_one_piece(\n",
    "                qaimage_output.q_input_ids, qaimage_output.a_input_ids\n",
    "            )\n",
    "            max_input_len_list.append(temp_input_ids.shape[1])\n",
    "\n",
    "            input_ids_list.append(temp_input_ids)\n",
    "            labels_list.append(temp_labels)\n",
    "            pixel_values_list.append(qaimage_output.pixel_values)\n",
    "        max_input_len = max(max_input_len_list)\n",
    "        # 对齐token的长度\n",
    "        input_ids = []\n",
    "        for index, value in enumerate(input_ids_list):\n",
    "            new_value = torch.concat(\n",
    "                [\n",
    "                    torch.full(\n",
    "                        size=(1, max_input_len - max_input_len_list[index]),\n",
    "                        fill_value=self.pad_token_ids,\n",
    "                    ),\n",
    "                    value,\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            input_ids.append(new_value)\n",
    "        labels = []\n",
    "        for index, value in enumerate(labels_list):\n",
    "            labels.append(\n",
    "                torch.concat(\n",
    "                    [\n",
    "                        torch.full(\n",
    "                            size=(1, max_input_len - max_input_len_list[index]),\n",
    "                            fill_value=self.ignore_index,\n",
    "                        ),\n",
    "                        value,\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "            )\n",
    "        input_ids = torch.concat(input_ids, axis=0)\n",
    "        labels = torch.concat(labels, axis=0)\n",
    "        pixel_values = torch.concat(pixel_values_list, axis=0)\n",
    "\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        attention_mask[input_ids == self.pad_token_ids] = 0 # 将填充的置为0\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_process.tokenizer.pad_token_ids,llava_process.tokenizer.eos_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6dd57ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlmc = TrainLlavaModelCollator(llava_process,-100)\n",
    "d = tlmc([test_llavadataset[23],test_llavadataset[100]])\n",
    "d[\"attention_mask\"]\n",
    "# d = tlmc.convert_one_piece(c.q_input_ids,c.a_input_ids)\n",
    "# d[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85852451",
   "metadata": {},
   "source": [
    "## 测试处理的数据，模型是否能够处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a84a5",
   "metadata": {},
   "source": [
    "### 1、模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9609d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a5241b716a4704b6372087794c04dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor,LlavaProcessor,LlavaForConditionalGeneration\n",
    "\n",
    "\n",
    "\n",
    "llava_model_path = \"models/llava_clip-L-14-336_Qwen1.5-1.8B\"\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(llava_model_path,dtype=torch.bfloat16,device_map=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f7f7e",
   "metadata": {},
   "source": [
    "### 2、测试多batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3a1a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tk in d.keys():\n",
    "    d[tk] = d[tk].to(llava_model.device)\n",
    "model_output = llava_model(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcbae8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.0397, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
