{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6deaf3",
   "metadata": {},
   "source": [
    "# 创建一个Llava模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42a62e",
   "metadata": {},
   "source": [
    "## 下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631b0aa",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "hf download openai/clip-vit-large-patch14-336 --local-dir ../../../../models/CLIP/clip-vit-large-patch14-336\n",
    "hf download Qwen/Qwen1.5-4B-Chat --local-dir ../../../models/Qwen/Qwen1.5-4B-Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53e34a",
   "metadata": {},
   "source": [
    "## Llava初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2126a",
   "metadata": {},
   "source": [
    "### 将clip模型和llm模型的config拿出来，初始化一个Llava model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9840d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlavaForConditionalGeneration,LlavaConfig,LlavaProcessor,\n",
    "    CLIPVisionModel,CLIPVisionConfig,CLIPImageProcessor,\n",
    "    Qwen2ForCausalLM,Qwen2Config,Qwen2Tokenizer\n",
    "    )\n",
    "import torch\n",
    "clip_model_path = \"./models/clip-vit-large-patch14-336\"\n",
    "qwen_model_path = \"./models/Qwen1.5-1.8B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fdecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载视觉模型\n",
    "vision_model = CLIPVisionModel.from_pretrained(clip_model_path)\n",
    "vision_config = CLIPVisionConfig.from_pretrained(clip_model_path)\n",
    "image_processor = CLIPImageProcessor.from_pretrained(clip_model_path)\n",
    "\n",
    "# 加载语言模型\n",
    "qwen_model = Qwen2ForCausalLM.from_pretrained(qwen_model_path)\n",
    "qwen_config = Qwen2Config.from_pretrained(qwen_model_path)\n",
    "qwen_tokenizer = Qwen2Tokenizer.from_pretrained(qwen_model_path)\n",
    "\n",
    "# 添加特殊token\n",
    "qwen_tokenizer.add_special_tokens({\"additional_special_tokens\":qwen_tokenizer.additional_special_tokens+[\"<image>\"]})\n",
    "\n",
    "llava_config = LlavaConfig(\n",
    "    vision_config=vision_config,\n",
    "    text_config=qwen_config,\n",
    "    ignore_index=-100,\n",
    "    image_token_index=151646\n",
    ")\n",
    "# 使用从配置初始化 Llava 模型结构\n",
    "llava_model = LlavaForConditionalGeneration(llava_config)\n",
    "llava_processor = LlavaProcessor(\n",
    "    image_processor=image_processor,\n",
    "    tokenizer=qwen_tokenizer,\n",
    "    num_additional_image_tokens=1,\n",
    "    patch_size=vision_config.patch_size,\n",
    "    vision_feature_select_strategy=\"default\" # 需要指定，否则tokens的维度与image_feature的维度对不上\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f717e",
   "metadata": {},
   "source": [
    "### 复制权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c0b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_model.model.vision_tower = vision_model\n",
    "llava_model.model.language_model = qwen_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8e9af",
   "metadata": {},
   "source": [
    "### 查看权重是否赋值成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb2328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0271,  0.0280])\n",
      "tensor([-0.0271,  0.0280])\n"
     ]
    }
   ],
   "source": [
    "print(llava_model.model.language_model.embed_tokens.weight.data[0,:2])\n",
    "print(qwen_model.model.embed_tokens.weight.data[0,:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda92bd6",
   "metadata": {},
   "source": [
    "### 复制pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc20968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_model.config.pad_token_id = qwen_tokenizer.pad_token_id\n",
    "llava_model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0b6ab",
   "metadata": {},
   "source": [
    "### 保存模型和相关的配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6336c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/llava_clip-L-14-336_Qwen1.5-1.8B/processor_config.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_model.save_pretrained(\"models/llava_clip-L-14-336_Qwen1.5-1.8B\")\n",
    "llava_processor.save_pretrained(\"models/llava_clip-L-14-336_Qwen1.5-1.8B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105bfe20",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ebe3283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0611945ce5d54f47ac9d2dac54612326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor,LlavaForConditionalGeneration,LlavaProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "llava_mode_path = \"models/llava_clip-L-14-336_Qwen1.5-1.8B\"\n",
    "\n",
    "llava_processor = LlavaProcessor.from_pretrained(llava_mode_path, use_fast=True)\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    llava_mode_path, device_map=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b1e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image>\n",
      "What are these?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "老化ourcemigate QHBoxLayout suggest锂电池ростយ mRNA switchesalto Jews mRNA}\"\n",
      "\n",
      " watchesourcem Firearmsukkan Preserve}\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "prompt_text = \"<image>\\nWhat are these?\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_text},\n",
    "]\n",
    "prompt = llava_processor.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "image_path = \"test.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "\n",
    "inputs = llava_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = llava_model.generate(**inputs, max_new_tokens=15)\n",
    "llava_processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "for tk in inputs.keys():\n",
    "    inputs[tk] = inputs[tk].to(llava_model.device)\n",
    "generate_ids = llava_model.generate(**inputs, max_new_tokens=20)\n",
    "gen_text = llava_processor.batch_decode(\n",
    "    generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(gen_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
